{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FB_featurization.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"62otSw_YH5L6","colab_type":"text"},"cell_type":"markdown","source":["<p style=\"font-size:32px;text-align:center\"> <b>Social network Graph Link Prediction - Facebook Challenge</b> </p>"]},{"metadata":{"id":"XhqOvUnBH5MD","colab_type":"code","colab":{}},"cell_type":"code","source":["#Importing Libraries\n","# please do go through this python notebook: \n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import csv\n","import pandas as pd#pandas to create small dataframes \n","import datetime #Convert to unix time\n","import time #Convert to unix time\n","# if numpy is not installed already : pip3 install numpy\n","import numpy as np#Do aritmetic operations on arrays\n","# matplotlib: used to plot graphs\n","import matplotlib\n","import matplotlib.pylab as plt\n","import seaborn as sns#Plots\n","from matplotlib import rcParams#Size of plots  \n","from sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\n","import math\n","import pickle\n","import os\n","# to install xgboost: pip3 install xgboost\n","import xgboost as xgb\n","\n","import warnings\n","import networkx as nx\n","import pdb\n","import pickle\n","from pandas import HDFStore,DataFrame\n","from pandas import read_hdf\n","from scipy.sparse.linalg import svds, eigs\n","import gc\n","from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u-ijLM9QH5MX","colab_type":"text"},"cell_type":"markdown","source":["# 1. Reading Data"]},{"metadata":{"id":"VBATsrrtH5Md","colab_type":"code","colab":{},"outputId":"0e52aa63-5b81-4026-bddc-8ecb17499264"},"cell_type":"code","source":["if os.path.isfile('data/after_eda/train_pos_after_eda.csv'):\n","    train_graph=nx.read_edgelist('data/after_eda/train_pos_after_eda.csv',delimiter=',',create_using=nx.DiGraph(),nodetype=int)\n","    print(nx.info(train_graph))\n","else:\n","    print(\"please run the FB_EDA.ipynb or download the files from drive\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Name: \n","Type: DiGraph\n","Number of nodes: 1780722\n","Number of edges: 7550015\n","Average in degree:   4.2399\n","Average out degree:   4.2399\n"],"name":"stdout"}]},{"metadata":{"id":"EXYdb7rkH5NB","colab_type":"text"},"cell_type":"markdown","source":["# 2. Similarity measures"]},{"metadata":{"id":"b1gBzXneH5NG","colab_type":"text"},"cell_type":"markdown","source":["## 2.1 Jaccard Distance:\n","http://www.statisticshowto.com/jaccard-index/"]},{"metadata":{"id":"ZDFxiTiBH5NL","colab_type":"text"},"cell_type":"markdown","source":["\\begin{equation}\n","j = \\frac{|X\\cap Y|}{|X \\cup Y|} \n","\\end{equation}"]},{"metadata":{"id":"ieVxbl4IH5NP","colab_type":"code","colab":{}},"cell_type":"code","source":["#for followees\n","def jaccard_for_followees(a,b):\n","    try:\n","        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n","            return 0\n","        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))/\\\n","                                    (len(set(train_graph.successors(a)).union(set(train_graph.successors(b)))))\n","    except:\n","        return 0\n","    return sim"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y6mW9DU1H5NZ","colab_type":"code","colab":{},"outputId":"39787d3a-46a2-4c8c-a2de-7de7f11addc0"},"cell_type":"code","source":["#one test case\n","print(jaccard_for_followees(273084,1505602))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.0\n"],"name":"stdout"}]},{"metadata":{"id":"W5Ihmp1tH5Ns","colab_type":"code","colab":{},"outputId":"78c14c91-1fa2-4043-a4bf-fd12d398914b"},"cell_type":"code","source":["#node 1635354 not in graph \n","print(jaccard_for_followees(273084,1505602))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.0\n"],"name":"stdout"}]},{"metadata":{"id":"-ztxkXaOH5N4","colab_type":"code","colab":{}},"cell_type":"code","source":["#for followers\n","def jaccard_for_followers(a,b):\n","    try:\n","        if len(set(train_graph.predecessors(a))) == 0  | len(set(g.predecessors(b))) == 0:\n","            return 0\n","        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))/\\\n","                                 (len(set(train_graph.predecessors(a)).union(set(train_graph.predecessors(b)))))\n","        return sim\n","    except:\n","        return 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cqHWnxU2H5OB","colab_type":"code","colab":{},"outputId":"1f98a342-cdc1-4938-ca1e-7489e95fa13a"},"cell_type":"code","source":["print(jaccard_for_followers(273084,470294))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"metadata":{"id":"0VvaqMl1H5OU","colab_type":"code","colab":{},"outputId":"0a3fa9d7-439c-4f30-9080-89de94ff6302"},"cell_type":"code","source":["#node 1635354 not in graph \n","print(jaccard_for_followees(669354,1635354))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"metadata":{"id":"4lp2dOgDH5Og","colab_type":"text"},"cell_type":"markdown","source":["## 2.2 Cosine distance (Otsuka-Ochiai coefficient)"]},{"metadata":{"id":"SB2OadcxH5Oj","colab_type":"text"},"cell_type":"markdown","source":["\\begin{equation}\n","CosineDistance = \\frac{|X\\cap Y|}{SQRT(|X|\\cdot|Y|)} \n","\\end{equation}"]},{"metadata":{"id":"wjj4Wo5LH5Om","colab_type":"code","colab":{}},"cell_type":"code","source":["#for followees\n","def cosine_for_followees(a,b):\n","    try:\n","        if len(set(train_graph.successors(a))) == 0  | len(set(train_graph.successors(b))) == 0:\n","            return 0\n","        sim = (len(set(train_graph.successors(a)).intersection(set(train_graph.successors(b)))))/\\\n","                                    (math.sqrt(len(set(train_graph.successors(a)))*len((set(train_graph.successors(b))))))\n","        return sim\n","    except:\n","        return 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pBw4HxUmH5Ox","colab_type":"code","colab":{},"outputId":"aef4b1fd-ee70-4883-c61d-87242cadc31f"},"cell_type":"code","source":["print(cosine_for_followees(273084,1505602))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.0\n"],"name":"stdout"}]},{"metadata":{"id":"S-WKFHe1H5PD","colab_type":"code","colab":{},"outputId":"e6de1585-a9c2-4afd-c03e-6aaabc5c6232"},"cell_type":"code","source":["print(cosine_for_followees(273084,1635354))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"metadata":{"id":"8n_nh5FnH5PR","colab_type":"code","colab":{}},"cell_type":"code","source":["def cosine_for_followers(a,b):\n","    try:\n","        \n","        if len(set(train_graph.predecessors(a))) == 0  | len(set(train_graph.predecessors(b))) == 0:\n","            return 0\n","        sim = (len(set(train_graph.predecessors(a)).intersection(set(train_graph.predecessors(b)))))/\\\n","                                     (math.sqrt(len(set(train_graph.predecessors(a))))*(len(set(train_graph.predecessors(b)))))\n","        return sim\n","    except:\n","        return 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XVtJb-7qH5Pb","colab_type":"code","colab":{},"outputId":"48d0fa8e-7678-42f2-a73c-ae72b5fafcc1"},"cell_type":"code","source":["print(cosine_for_followers(2,470294))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.02886751345948129\n"],"name":"stdout"}]},{"metadata":{"id":"wy1M1fvcH5Pw","colab_type":"code","colab":{},"outputId":"1af39ebd-dbd6-4b3f-bb2c-04d1f6d3deb4"},"cell_type":"code","source":["print(cosine_for_followers(669354,1635354))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"metadata":{"id":"cicN-uECH5QI","colab_type":"text"},"cell_type":"markdown","source":["## 3. Ranking Measures"]},{"metadata":{"id":"11mEON8CH5QO","colab_type":"text"},"cell_type":"markdown","source":["https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html\n","\n","PageRank computes a ranking of the nodes in the graph G based on the structure of the incoming links.\n","\n","<img src='PageRanks-Example.jpg'/>\n","\n","Mathematical PageRanks for a simple network, expressed as percentages. (Google uses a logarithmic scale.) Page C has a higher PageRank than Page E, even though there are fewer links to C; the one link to C comes from an important page and hence is of high value. If web surfers who start on a random page have an 85% likelihood of choosing a random link from the page they are currently visiting, and a 15% likelihood of jumping to a page chosen at random from the entire web, they will reach Page E 8.1% of the time. <b>(The 15% likelihood of jumping to an arbitrary page corresponds to a damping factor of 85%.) Without damping, all web surfers would eventually end up on Pages A, B, or C, and all other pages would have PageRank zero. In the presence of damping, Page A effectively links to all pages in the web, even though it has no outgoing links of its own.</b>"]},{"metadata":{"id":"pG4bm4dRH5QU","colab_type":"text"},"cell_type":"markdown","source":["## 3.1 Page Rank\n","\n","https://en.wikipedia.org/wiki/PageRank\n"]},{"metadata":{"id":"zEcgIKRLH5QY","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isfile('data/fea_sample/page_rank.p'):\n","    pr = nx.pagerank(train_graph, alpha=0.85)\n","    pickle.dump(pr,open('data/fea_sample/page_rank.p','wb'))\n","else:\n","    pr = pickle.load(open('data/fea_sample/page_rank.p','rb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"01C0JtM1H5Qg","colab_type":"code","colab":{},"outputId":"ecae2dc0-e509-47c5-9184-ce5b892a9d86"},"cell_type":"code","source":["print('min',pr[min(pr, key=pr.get)])\n","print('max',pr[max(pr, key=pr.get)])\n","print('mean',float(sum(pr.values())) / len(pr))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["min 1.6556497245737814e-07\n","max 2.7098251341935827e-05\n","mean 5.615699699389075e-07\n"],"name":"stdout"}]},{"metadata":{"id":"mOhs2nBjH5Qt","colab_type":"code","colab":{},"outputId":"37c99421-097c-4c70-c4c6-bdbbaf232275"},"cell_type":"code","source":["#for imputing to nodes which are not there in Train data\n","mean_pr = float(sum(pr.values())) / len(pr)\n","print(mean_pr)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["5.615699699389075e-07\n"],"name":"stdout"}]},{"metadata":{"id":"SPH84_OHH5Q3","colab_type":"text"},"cell_type":"markdown","source":["# 4. Other Graph-based Features"]},{"metadata":{"id":"RxtIp1-eH5Q8","colab_type":"text"},"cell_type":"markdown","source":["## 4.1 Shortest path:"]},{"metadata":{"id":"1j3JX8ZCH5RB","colab_type":"text"},"cell_type":"markdown","source":["Getting Shortest path between two nodes, if nodes have an edge i.e, trivially connected then we are removing that edge and calculating the shortest path. "]},{"metadata":{"id":"Ndg19WBUH5RD","colab_type":"code","colab":{}},"cell_type":"code","source":["#if has direct edge then deleting that edge and calculating shortest path\n","def compute_shortest_path_length(a,b):\n","    p=-1\n","    try:\n","        if train_graph.has_edge(a,b):\n","            train_graph.remove_edge(a,b)\n","            p= nx.shortest_path_length(train_graph,source=a,target=b)\n","            train_graph.add_edge(a,b)\n","        else:\n","            p= nx.shortest_path_length(train_graph,source=a,target=b)\n","        return p\n","    except:\n","        return -1\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"EFXwiIJfH5RM","colab_type":"code","colab":{},"outputId":"5f0cc7f5-9c69-4cf2-a41d-cba0a5d2f186"},"cell_type":"code","source":["#testing\n","compute_shortest_path_length(77697, 826021)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"16sHHoktH5Ra","colab_type":"code","colab":{},"outputId":"3cf03e2f-27a2-4521-9dca-025b103cf1a0"},"cell_type":"code","source":["#testing\n","compute_shortest_path_length(669354,1635354)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-1"]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"bkC2nc_0H5Rl","colab_type":"text"},"cell_type":"markdown","source":["## 4.2 Checking for same weakly conected component (Community)"]},{"metadata":{"id":"6-C3ckS0H5Rn","colab_type":"code","colab":{}},"cell_type":"code","source":["#getting weekly connected edges from graph \n","wcc=list(nx.weakly_connected_components(train_graph))\n","def belongs_to_same_wcc(a,b):\n","    index = []\n","    if train_graph.has_edge(b,a):\n","        return 1\n","    if train_graph.has_edge(a,b):\n","            for i in wcc:\n","                if a in i:\n","                    index= i\n","                    break\n","            if (b in index):\n","                train_graph.remove_edge(a,b)\n","                if compute_shortest_path_length(a,b)==-1:\n","                    train_graph.add_edge(a,b)\n","                    return 0\n","                else:\n","                    train_graph.add_edge(a,b)\n","                    return 1\n","            else:\n","                return 0\n","    else:\n","            for i in wcc:\n","                if a in i:\n","                    index= i\n","                    break\n","            if(b in index):\n","                return 1\n","            else:\n","                return 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nF-TNQaWH5Ry","colab_type":"code","colab":{},"outputId":"d523288a-9591-4307-c064-29378ccd7232"},"cell_type":"code","source":["belongs_to_same_wcc(861, 1659750)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":24}]},{"metadata":{"id":"D5DL798uH5SF","colab_type":"code","colab":{},"outputId":"3b9f77c9-73db-4a5a-e76f-36748d70dc57"},"cell_type":"code","source":["belongs_to_same_wcc(669354,1635354)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":25}]},{"metadata":{"id":"e8UwxmIfH5SO","colab_type":"text"},"cell_type":"markdown","source":["## 4.3 Adamic/Adar Index:\n","Adamic/Adar measures is defined as inverted sum of degrees of common neighbours for given two vertices.\n","$$A(x,y)=\\sum_{u \\in N(x) \\cap N(y)}\\frac{1}{log(|N(u)|)}$$"]},{"metadata":{"id":"ku_0Nj70H5SV","colab_type":"code","colab":{}},"cell_type":"code","source":["#adar index\n","def calc_adar_in(a,b):\n","    sum=0\n","    try:\n","        n=list(set(train_graph.successors(a)).intersection(set(train_graph.successors(b))))\n","        if len(n)!=0:\n","            for i in n:\n","                sum=sum+(1/np.log10(len(list(train_graph.predecessors(i)))))\n","            return sum\n","        else:\n","            return 0\n","    except:\n","        return 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0ooF6aDmH5Se","colab_type":"code","colab":{},"outputId":"bfd20a16-bdb8-48a1-bd5f-5d54282e84cf"},"cell_type":"code","source":["calc_adar_in(1,189226)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":27}]},{"metadata":{"id":"NaYwISOaH5Su","colab_type":"code","colab":{},"outputId":"54126a8b-e5c9-4be2-fd50-0ee57c0aa63c"},"cell_type":"code","source":["calc_adar_in(669354,1635354)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":28}]},{"metadata":{"id":"wBRk_hp1H5S3","colab_type":"text"},"cell_type":"markdown","source":["## 4.4 Does the person follow back?"]},{"metadata":{"id":"_TBe_FB6H5S5","colab_type":"code","colab":{}},"cell_type":"code","source":["def follows_back(a,b):\n","    if train_graph.has_edge(b,a):\n","        return 1\n","    else:\n","        return 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T1DKbxfSH5TD","colab_type":"code","colab":{},"outputId":"43444353-aff4-4d45-95c1-4b53b80fc96c"},"cell_type":"code","source":["follows_back(1,189226)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":30}]},{"metadata":{"id":"saDkcmL4H5TS","colab_type":"code","colab":{},"outputId":"445ba33e-3781-4f05-a52c-cc49641a17c5"},"cell_type":"code","source":["follows_back(669354,1635354)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":31}]},{"metadata":{"id":"6bY7o3B8H5Th","colab_type":"text"},"cell_type":"markdown","source":["## 4.5 Katz Centrality:\n","https://en.wikipedia.org/wiki/Katz_centrality\n","\n","https://www.geeksforgeeks.org/katz-centrality-centrality-measure/\n"," "]},{"metadata":{"id":"sEvabXQQH5Tj","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isfile('data/fea_sample/katz.p'):\n","    katz = nx.katz.katz_centrality(train_graph,alpha=0.005,beta=1)\n","    pickle.dump(katz,open('data/fea_sample/katz.p','wb'))\n","else:\n","    katz = pickle.load(open('data/fea_sample/katz.p','rb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ohRf3aySH5Tv","colab_type":"code","colab":{},"outputId":"5dac314e-d5b6-4a39-c898-239fb9fb8713"},"cell_type":"code","source":["print('min',katz[min(katz, key=katz.get)])\n","print('max',katz[max(katz, key=katz.get)])\n","print('mean',float(sum(katz.values())) / len(katz))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["min 0.0007313532484065916\n","max 0.003394554981699122\n","mean 0.0007483800935562018\n"],"name":"stdout"}]},{"metadata":{"id":"2eVaavuzH5UD","colab_type":"code","colab":{},"outputId":"cb54144b-f4c7-4629-d1ea-9f11502b56f1"},"cell_type":"code","source":["mean_katz = float(sum(katz.values())) / len(katz)\n","print(mean_katz)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.0007483800935562018\n"],"name":"stdout"}]},{"metadata":{"id":"0fc12RUvH5UU","colab_type":"text"},"cell_type":"markdown","source":["## 4.6 HITS Score\n","The HITS algorithm computes two numbers for a node. Authorities estimates the node value based on the incoming links. Hubs estimates the node value based on outgoing links.\n","\n","https://en.wikipedia.org/wiki/HITS_algorithm"]},{"metadata":{"id":"_yvxkXBjH5UY","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isfile('data/fea_sample/hits.p'):\n","    hits = nx.hits(train_graph, max_iter=100, tol=1e-08, nstart=None, normalized=True)\n","    pickle.dump(hits,open('data/fea_sample/hits.p','wb'))\n","else:\n","    hits = pickle.load(open('data/fea_sample/hits.p','rb'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c25FZ_cqH5Ul","colab_type":"code","colab":{},"outputId":"8f559e0a-3d69-41c0-e1dc-3745871ea029"},"cell_type":"code","source":["print('min',hits[0][min(hits[0], key=hits[0].get)])\n","print('max',hits[0][max(hits[0], key=hits[0].get)])\n","print('mean',float(sum(hits[0].values())) / len(hits[0]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["min 0.0\n","max 0.004868653378780953\n","mean 5.615699699344123e-07\n"],"name":"stdout"}]},{"metadata":{"id":"8bW9ORP2H5Ux","colab_type":"text"},"cell_type":"markdown","source":["# 5. Featurization"]},{"metadata":{"id":"TkHR8y56H5U4","colab_type":"text"},"cell_type":"markdown","source":["## 5. 1 Reading a sample of Data from both train and test"]},{"metadata":{"id":"TBd9_Ph9H5U7","colab_type":"code","colab":{}},"cell_type":"code","source":["import random\n","if os.path.isfile('data/after_eda/train_after_eda.csv'):\n","    filename = \"data/after_eda/train_after_eda.csv\"\n","    # you uncomment this line, if you dont know the lentgh of the file name\n","    # here we have hardcoded the number of lines as 15100030\n","    # n_train = sum(1 for line in open(filename)) #number of records in file (excludes header)\n","    n_train =  15100028\n","    s = 100000 #desired sample size\n","    skip_train = sorted(random.sample(range(1,n_train+1),n_train-s))\n","    #https://stackoverflow.com/a/22259008/4084039"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k0akW1SRH5VB","colab_type":"code","colab":{}},"cell_type":"code","source":["if os.path.isfile('data/after_eda/train_after_eda.csv'):\n","    filename = \"data/after_eda/test_after_eda.csv\"\n","    # you uncomment this line, if you dont know the lentgh of the file name\n","    # here we have hardcoded the number of lines as 3775008\n","    # n_test = sum(1 for line in open(filename)) #number of records in file (excludes header)\n","    n_test = 3775006\n","    s = 50000 #desired sample size\n","    skip_test = sorted(random.sample(range(1,n_test+1),n_test-s))\n","    #https://stackoverflow.com/a/22259008/4084039"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PxcRCJWZH5VH","colab_type":"code","colab":{},"outputId":"25725ed7-3cbc-4947-e629-c20ec4f868f4"},"cell_type":"code","source":["print(\"Number of rows in the train data file:\", n_train)\n","print(\"Number of rows we are going to elimiate in train data are\",len(skip_train))\n","print(\"Number of rows in the test data file:\", n_test)\n","print(\"Number of rows we are going to elimiate in test data are\",len(skip_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of rows in the train data file: 15100028\n","Number of rows we are going to elimiate in train data are 15000028\n","Number of rows in the test data file: 3775006\n","Number of rows we are going to elimiate in test data are 3725006\n"],"name":"stdout"}]},{"metadata":{"id":"LVKkVuuSH5VU","colab_type":"code","colab":{},"outputId":"632ed9a5-3b7e-4161-ac1c-97abb63f687b"},"cell_type":"code","source":["df_final_train = pd.read_csv('data/after_eda/train_after_eda.csv', skiprows=skip_train, names=['source_node', 'destination_node'])\n","df_final_train['indicator_link'] = pd.read_csv('data/train_y.csv', skiprows=skip_train, names=['indicator_link'])\n","print(\"Our train matrix size \",df_final_train.shape)\n","df_final_train.head(2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Our train matrix size  (100002, 3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source_node</th>\n","      <th>destination_node</th>\n","      <th>indicator_link</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>273084</td>\n","      <td>1505602</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>832016</td>\n","      <td>1543415</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   source_node  destination_node  indicator_link\n","0       273084           1505602               1\n","1       832016           1543415               1"]},"metadata":{"tags":[]},"execution_count":49}]},{"metadata":{"id":"xrb1Ok-uH5Vl","colab_type":"code","colab":{},"outputId":"4085f322-aa8e-46c9-82c7-c35079cae380"},"cell_type":"code","source":["df_final_test = pd.read_csv('data/after_eda/test_after_eda.csv', skiprows=skip_test, names=['source_node', 'destination_node'])\n","df_final_test['indicator_link'] = pd.read_csv('data/test_y.csv', skiprows=skip_test, names=['indicator_link'])\n","print(\"Our test matrix size \",df_final_test.shape)\n","df_final_test.head(2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Our test matrix size  (50002, 3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source_node</th>\n","      <th>destination_node</th>\n","      <th>indicator_link</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>848424</td>\n","      <td>784690</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>483294</td>\n","      <td>1255532</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   source_node  destination_node  indicator_link\n","0       848424            784690               1\n","1       483294           1255532               1"]},"metadata":{"tags":[]},"execution_count":50}]},{"metadata":{"id":"iyoVV0evH5V3","colab_type":"text"},"cell_type":"markdown","source":["## 5.2 Adding a set of features\n","\n","__we will create these each of these features for both train and test data points__\n","<ol>\n","<li>jaccard_followers</li>\n","<li>jaccard_followees</li>\n","<li>cosine_followers</li>\n","<li>cosine_followees</li>\n","<li>num_followers_s</li>\n","<li>num_followees_s</li>\n","<li>num_followers_d</li>\n","<li>num_followees_d</li>\n","<li>inter_followers</li>\n","<li>inter_followees</li>\n","</ol>"]},{"metadata":{"id":"tKufd9cGH5V6","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isfile('data/fea_sample/storage_sample_stage1.h5'):\n","    #mapping jaccrd followers to train and test data\n","    df_final_train['jaccard_followers'] = df_final_train.apply(lambda row:\n","                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n","    df_final_test['jaccard_followers'] = df_final_test.apply(lambda row:\n","                                            jaccard_for_followers(row['source_node'],row['destination_node']),axis=1)\n","\n","    #mapping jaccrd followees to train and test data\n","    df_final_train['jaccard_followees'] = df_final_train.apply(lambda row:\n","                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n","    df_final_test['jaccard_followees'] = df_final_test.apply(lambda row:\n","                                            jaccard_for_followees(row['source_node'],row['destination_node']),axis=1)\n","    \n","\n","        #mapping jaccrd followers to train and test data\n","    df_final_train['cosine_followers'] = df_final_train.apply(lambda row:\n","                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n","    df_final_test['cosine_followers'] = df_final_test.apply(lambda row:\n","                                            cosine_for_followers(row['source_node'],row['destination_node']),axis=1)\n","\n","    #mapping jaccrd followees to train and test data\n","    df_final_train['cosine_followees'] = df_final_train.apply(lambda row:\n","                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)\n","    df_final_test['cosine_followees'] = df_final_test.apply(lambda row:\n","                                            cosine_for_followees(row['source_node'],row['destination_node']),axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ixokMkq1H5WJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def compute_features_stage1(df_final):\n","    #calculating no of followers followees for source and destination\n","    #calculating intersection of followers and followees for source and destination\n","    num_followers_s=[]\n","    num_followees_s=[]\n","    num_followers_d=[]\n","    num_followees_d=[]\n","    inter_followers=[]\n","    inter_followees=[]\n","    for i,row in df_final.iterrows():\n","        try:\n","            s1=set(train_graph.predecessors(row['source_node']))\n","            s2=set(train_graph.successors(row['source_node']))\n","        except:\n","            s1 = set()\n","            s2 = set()\n","        try:\n","            d1=set(train_graph.predecessors(row['destination_node']))\n","            d2=set(train_graph.successors(row['destination_node']))\n","        except:\n","            d1 = set()\n","            d2 = set()\n","        num_followers_s.append(len(s1))\n","        num_followees_s.append(len(s2))\n","\n","        num_followers_d.append(len(d1))\n","        num_followees_d.append(len(d2))\n","\n","        inter_followers.append(len(s1.intersection(d1)))\n","        inter_followees.append(len(s2.intersection(d2)))\n","    \n","    return num_followers_s, num_followers_d, num_followees_s, num_followees_d, inter_followers, inter_followees"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nMfccyovH5Wa","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isfile('data/fea_sample/storage_sample_stage1.h5'):\n","    df_final_train['num_followers_s'], df_final_train['num_followers_s'], \\\n","    df_final_train['num_followees_s'], df_final_train['num_followees_d'], \\\n","    df_final_train['inter_followers'], df_final_train['inter_followees']= compute_features_stage1(df_final_train)\n","    \n","    df_final_test['num_followers_s'], df_final_test['num_followers_s'], \\\n","    df_final_test['num_followees_s'], df_final_test['num_followees_d'], \\\n","    df_final_test['inter_followers'], df_final_test['inter_followees']= compute_features_stage1(df_final_test)\n","    \n","    hdf = HDFStore('data/fea_sample/storage_sample_stage1.h5')\n","    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n","    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n","    hdf.close()\n","else:\n","    df_final_train = read_hdf('data/fea_sample/storage_sample_stage1.h5', 'train_df',mode='r')\n","    df_final_test = read_hdf('data/fea_sample/storage_sample_stage1.h5', 'test_df',mode='r')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sBMBhbhSH5Wn","colab_type":"text"},"cell_type":"markdown","source":["## 5.3 Adding new set of features\n","\n","__we will create these each of these features for both train and test data points__\n","<ol>\n","<li>adar index</li>\n","<li>is following back</li>\n","<li>belongs to same weakly connect components</li>\n","<li>shortest path between source and destination</li>\n","</ol>"]},{"metadata":{"id":"0kVgIMpPH5Wq","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isfile('data/fea_sample/storage_sample_stage2.h5'):\n","    #mapping adar index on train\n","    df_final_train['adar_index'] = df_final_train.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n","    #mapping adar index on test\n","    df_final_test['adar_index'] = df_final_test.apply(lambda row: calc_adar_in(row['source_node'],row['destination_node']),axis=1)\n","\n","    #--------------------------------------------------------------------------------------------------------\n","    #mapping followback or not on train\n","    df_final_train['follows_back'] = df_final_train.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n","\n","    #mapping followback or not on test\n","    df_final_test['follows_back'] = df_final_test.apply(lambda row: follows_back(row['source_node'],row['destination_node']),axis=1)\n","\n","    #--------------------------------------------------------------------------------------------------------\n","    #mapping same component of wcc or not on train\n","    df_final_train['same_comp'] = df_final_train.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n","\n","    ##mapping same component of wcc or not on train\n","    df_final_test['same_comp'] = df_final_test.apply(lambda row: belongs_to_same_wcc(row['source_node'],row['destination_node']),axis=1)\n","    \n","    #--------------------------------------------------------------------------------------------------------\n","    #mapping shortest path on train \n","    df_final_train['shortest_path'] = df_final_train.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n","    #mapping shortest path on test\n","    df_final_test['shortest_path'] = df_final_test.apply(lambda row: compute_shortest_path_length(row['source_node'],row['destination_node']),axis=1)\n","\n","    hdf = HDFStore('data/fea_sample/storage_sample_stage2.h5')\n","    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n","    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n","    hdf.close()\n","else:\n","    df_final_train = read_hdf('data/fea_sample/storage_sample_stage2.h5', 'train_df',mode='r')\n","    df_final_test = read_hdf('data/fea_sample/storage_sample_stage2.h5', 'test_df',mode='r')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hKVb6Aa8H5Wv","colab_type":"text"},"cell_type":"markdown","source":["## 5.4 Adding new set of features\n","\n","__we will create these each of these features for both train and test data points__\n","<ol>\n","<li>Weight Features\n","    <ul>\n","        <li>weight of incoming edges</li>\n","        <li>weight of outgoing edges</li>\n","        <li>weight of incoming edges + weight of outgoing edges</li>\n","        <li>weight of incoming edges * weight of outgoing edges</li>\n","        <li>2*weight of incoming edges + weight of outgoing edges</li>\n","        <li>weight of incoming edges + 2*weight of outgoing edges</li>\n","    </ul>\n","</li>\n","<li>Page Ranking of source</li>\n","<li>Page Ranking of dest</li>\n","<li>katz of source</li>\n","<li>katz of dest</li>\n","<li>hubs of source</li>\n","<li>hubs of dest</li>\n","<li>authorities_s of source</li>\n","<li>authorities_s of dest</li>\n","</ol>"]},{"metadata":{"id":"KcZlicyFH5Wx","colab_type":"text"},"cell_type":"markdown","source":["#### Weight Features"]},{"metadata":{"id":"MW1St5QuH5Wy","colab_type":"text"},"cell_type":"markdown","source":["In order to determine the similarity of nodes, an edge weight value was calculated between nodes. Edge weight decreases as the neighbor count goes up. Intuitively, consider one million people following a celebrity on a social network then chances are most of them never met each other or the celebrity. On the other hand, if a user has 30 contacts in his/her social network, the chances are higher that many of them know each other. \n","`credit` - Graph-based Features for Supervised Link Prediction\n","William Cukierski, Benjamin Hamner, Bo Yang"]},{"metadata":{"id":"Kb7R5dNgH5W0","colab_type":"text"},"cell_type":"markdown","source":["\\begin{equation}\n","W = \\frac{1}{\\sqrt{1+|X|}}\n","\\end{equation}"]},{"metadata":{"id":"8m1YL0PCH5W2","colab_type":"text"},"cell_type":"markdown","source":["it is directed graph so calculated Weighted in and Weighted out differently"]},{"metadata":{"id":"3M4fK6HnH5W3","colab_type":"code","colab":{},"outputId":"2f0fee03-384c-4791-9cdc-9f3a2b03c752"},"cell_type":"code","source":["#weight for source and destination of each link\n","Weight_in = {}\n","Weight_out = {}\n","for i in  tqdm(train_graph.nodes()):\n","    s1=set(train_graph.predecessors(i))\n","    w_in = 1.0/(np.sqrt(1+len(s1)))\n","    Weight_in[i]=w_in\n","    \n","    s2=set(train_graph.successors(i))\n","    w_out = 1.0/(np.sqrt(1+len(s2)))\n","    Weight_out[i]=w_out\n","    \n","#for imputing with mean\n","mean_weight_in = np.mean(list(Weight_in.values()))\n","mean_weight_out = np.mean(list(Weight_out.values()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 1780722/1780722 [00:11<00:00, 152682.24it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"j38itoGtH5XD","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isfile('data/fea_sample/storage_sample_stage3.h5'):\n","    #mapping to pandas train\n","    df_final_train['weight_in'] = df_final_train.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n","    df_final_train['weight_out'] = df_final_train.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n","\n","    #mapping to pandas test\n","    df_final_test['weight_in'] = df_final_test.destination_node.apply(lambda x: Weight_in.get(x,mean_weight_in))\n","    df_final_test['weight_out'] = df_final_test.source_node.apply(lambda x: Weight_out.get(x,mean_weight_out))\n","\n","\n","    #some features engineerings on the in and out weights\n","    df_final_train['weight_f1'] = df_final_train.weight_in + df_final_train.weight_out\n","    df_final_train['weight_f2'] = df_final_train.weight_in * df_final_train.weight_out\n","    df_final_train['weight_f3'] = (2*df_final_train.weight_in + 1*df_final_train.weight_out)\n","    df_final_train['weight_f4'] = (1*df_final_train.weight_in + 2*df_final_train.weight_out)\n","\n","    #some features engineerings on the in and out weights\n","    df_final_test['weight_f1'] = df_final_test.weight_in + df_final_test.weight_out\n","    df_final_test['weight_f2'] = df_final_test.weight_in * df_final_test.weight_out\n","    df_final_test['weight_f3'] = (2*df_final_test.weight_in + 1*df_final_test.weight_out)\n","    df_final_test['weight_f4'] = (1*df_final_test.weight_in + 2*df_final_test.weight_out)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U99n20zgH5XI","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isfile('data/fea_sample/storage_sample_stage3.h5'):\n","    \n","    #page rank for source and destination in Train and Test\n","    #if anything not there in train graph then adding mean page rank \n","    df_final_train['page_rank_s'] = df_final_train.source_node.apply(lambda x:pr.get(x,mean_pr))\n","    df_final_train['page_rank_d'] = df_final_train.destination_node.apply(lambda x:pr.get(x,mean_pr))\n","\n","    df_final_test['page_rank_s'] = df_final_test.source_node.apply(lambda x:pr.get(x,mean_pr))\n","    df_final_test['page_rank_d'] = df_final_test.destination_node.apply(lambda x:pr.get(x,mean_pr))\n","    #================================================================================\n","\n","    #Katz centrality score for source and destination in Train and test\n","    #if anything not there in train graph then adding mean katz score\n","    df_final_train['katz_s'] = df_final_train.source_node.apply(lambda x: katz.get(x,mean_katz))\n","    df_final_train['katz_d'] = df_final_train.destination_node.apply(lambda x: katz.get(x,mean_katz))\n","\n","    df_final_test['katz_s'] = df_final_test.source_node.apply(lambda x: katz.get(x,mean_katz))\n","    df_final_test['katz_d'] = df_final_test.destination_node.apply(lambda x: katz.get(x,mean_katz))\n","    #================================================================================\n","\n","    #Hits algorithm score for source and destination in Train and test\n","    #if anything not there in train graph then adding 0\n","    df_final_train['hubs_s'] = df_final_train.source_node.apply(lambda x: hits[0].get(x,0))\n","    df_final_train['hubs_d'] = df_final_train.destination_node.apply(lambda x: hits[0].get(x,0))\n","\n","    df_final_test['hubs_s'] = df_final_test.source_node.apply(lambda x: hits[0].get(x,0))\n","    df_final_test['hubs_d'] = df_final_test.destination_node.apply(lambda x: hits[0].get(x,0))\n","    #================================================================================\n","\n","    #Hits algorithm score for source and destination in Train and Test\n","    #if anything not there in train graph then adding 0\n","    df_final_train['authorities_s'] = df_final_train.source_node.apply(lambda x: hits[1].get(x,0))\n","    df_final_train['authorities_d'] = df_final_train.destination_node.apply(lambda x: hits[1].get(x,0))\n","\n","    df_final_test['authorities_s'] = df_final_test.source_node.apply(lambda x: hits[1].get(x,0))\n","    df_final_test['authorities_d'] = df_final_test.destination_node.apply(lambda x: hits[1].get(x,0))\n","    #================================================================================\n","\n","    hdf = HDFStore('data/fea_sample/storage_sample_stage3.h5')\n","    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n","    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n","    hdf.close()\n","else:\n","    df_final_train = read_hdf('data/fea_sample/storage_sample_stage3.h5', 'train_df',mode='r')\n","    df_final_test = read_hdf('data/fea_sample/storage_sample_stage3.h5', 'test_df',mode='r')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6ARGJchGH5XS","colab_type":"text"},"cell_type":"markdown","source":["## 5.5 Adding new set of features\n","\n","__we will create these each of these features for both train and test data points__\n","<ol>\n","<li>SVD features for both source and destination</li>\n","</ol>"]},{"metadata":{"id":"IVe-Vo9iH5XT","colab_type":"code","colab":{}},"cell_type":"code","source":["def svd(x, S):\n","    try:\n","        z = sadj_dict[x]\n","        return S[z]\n","    except:\n","        return [0,0,0,0,0,0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MgER8aZ0H5Xe","colab_type":"code","colab":{}},"cell_type":"code","source":["#for svd features to get feature vector creating a dict node val and inedx in svd vector\n","sadj_col = sorted(train_graph.nodes())\n","sadj_dict = { val:idx for idx,val in enumerate(sadj_col)}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XC9BSD-UH5Xj","colab_type":"code","colab":{}},"cell_type":"code","source":["Adj = nx.adjacency_matrix(train_graph,nodelist=sorted(train_graph.nodes())).asfptype()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tg5H-z0KH5Xo","colab_type":"code","colab":{},"outputId":"9a5d9e7a-093f-4983-89e8-92902898f060"},"cell_type":"code","source":["U, s, V = svds(Adj, k = 6)\n","print('Adjacency matrix Shape',Adj.shape)\n","print('U Shape',U.shape)\n","print('V Shape',V.shape)\n","print('s Shape',s.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Adjacency matrix Shape (1780722, 1780722)\n","U Shape (1780722, 6)\n","V Shape (6, 1780722)\n","s Shape (6,)\n"],"name":"stdout"}]},{"metadata":{"id":"EAiTAi02H5X3","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isfile('data/fea_sample/storage_sample_stage4.h5'):\n","    #===================================================================================================\n","    \n","    df_final_train[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n","    df_final_train.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n","    \n","    df_final_train[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n","    df_final_train.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n","    #===================================================================================================\n","    \n","    df_final_train[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n","    df_final_train.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n","\n","    df_final_train[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n","    df_final_train.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n","    #===================================================================================================\n","    \n","    df_final_test[['svd_u_s_1', 'svd_u_s_2','svd_u_s_3', 'svd_u_s_4', 'svd_u_s_5', 'svd_u_s_6']] = \\\n","    df_final_test.source_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n","    \n","    df_final_test[['svd_u_d_1', 'svd_u_d_2', 'svd_u_d_3', 'svd_u_d_4', 'svd_u_d_5','svd_u_d_6']] = \\\n","    df_final_test.destination_node.apply(lambda x: svd(x, U)).apply(pd.Series)\n","\n","    #===================================================================================================\n","    \n","    df_final_test[['svd_v_s_1','svd_v_s_2', 'svd_v_s_3', 'svd_v_s_4', 'svd_v_s_5', 'svd_v_s_6',]] = \\\n","    df_final_test.source_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n","\n","    df_final_test[['svd_v_d_1', 'svd_v_d_2', 'svd_v_d_3', 'svd_v_d_4', 'svd_v_d_5','svd_v_d_6']] = \\\n","    df_final_test.destination_node.apply(lambda x: svd(x, V.T)).apply(pd.Series)\n","    #===================================================================================================\n","\n","    hdf = HDFStore('data/fea_sample/storage_sample_stage4.h5')\n","    hdf.put('train_df',df_final_train, format='table', data_columns=True)\n","    hdf.put('test_df',df_final_test, format='table', data_columns=True)\n","    hdf.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S44UkQbfH5X8","colab_type":"code","colab":{}},"cell_type":"code","source":["# prepared and stored the data from machine learning models\n","# pelase check the FB_Models.ipynb"],"execution_count":0,"outputs":[]}]}